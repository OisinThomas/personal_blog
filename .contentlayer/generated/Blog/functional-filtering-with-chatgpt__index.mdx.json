{
  "title": "Functional Filtering with ChatGPT",
  "publishedAt": "2023-06-16T00:00:00.000Z",
  "updatedAt": "2023-06-16T00:00:00.000Z",
  "description": "LLMs still going brrr thanks to timesaving with JSON outputs",
  "image": {
    "filePath": "../public/profile.png",
    "relativeFilePath": "../../public/profile.png",
    "format": "png",
    "height": 1024,
    "width": 1024,
    "aspectRatio": 1,
    "blurhashDataUrl": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAYAAADED76LAAAA+UlEQVR4nDXOP0gCYRjH8be5hpaCqCUCawjiaqjBtSUInAoiqK2xvUEcWyIIClzaioY0EbzT/k0ph0UnSRiXkYKciHKTnoP6fn3h8APPAz+e3/CInm1Qz95Sf0/ilnJ8vT6QNeKkkvekHnUETgHv38SxnnD0C7anxwmtBCiaH+TNHKJbtcC1qaUvyUT2SEcO2NnQWJiaoVT8RUj3j4oRxQjNMTsm2F1fJTA5gTavsbS8hRh4TaxEFOhi59+Q7TInm2sEF4M0Wh31g/J8c6W20vnBc74p6zFOj8+RUvoFkGp8n/E7dehzdp1RiVHB9xLep5CIcXQYVsk3BHqjyD2wCBcLAAAAK3RFWHRDcmVhdGlvbiBUaW1lAFdlZCwgMDMgQXByIDIwMjQgMTI6MzA6MzUgR01U+CA3pwAAAC50RVh0U29mdHdhcmUAZ2l0aHViLmNvbS9tYXRtZW4vSW1hZ2VTY3JpcHQgdjEuMi4xNujZvYYAAAAASUVORK5CYII="
  },
  "isPublished": true,
  "author": "Oisín Thomas Morrin",
  "tags": [
    "Tech",
    "ChatGPT",
    "OpenAI",
    "AI",
    "\r"
  ],
  "majorTag": "Thoughts",
  "subTag": "Tech",
  "language": "en",
  "body": {
    "raw": "\r\nAnother week, another slew of AI updates. In particular, this week has seen a very exciting new development from OpenAI: function calling. Their post about it was [rather uninspiring](https://platform.openai.com/docs/guides/gpt/function-calling), so I wanted to give it a closer look and show you why it came about, why it matters, and how it is really useful.\r\n\r\n## Up to Now\r\n\r\nChat completions using OpenAI have taken the world by storm, with \"Ask GPT\" trending on Twitter and much fear and excitement for what LLMs will mean for the world of work—especially in the realm of content creation. And, that isn't really surprising because, up to now, the most obvious use cases for the chat models have been that: conversation and content media creation.\r\n\r\nThe reach of the technology stepped forward with the advent of ChatGPT plugins—allowing it to access the web from within the scope of the chat window for the first time, and not just through the developer's API.\r\n\r\n## The Information to Noise Ratio\r\n\r\nBut, a large problem remained: it is very difficult to extract information from a conversation and these Chat models were rather fallible when it came to extracting key pieces of information from text in a predictable way.\r\n\r\nAn early fix for this was asking the model to respond in JSON format which is common across the internet itself, basically key-value pairs. For example:\r\n\r\n```json\r\n{\r\n  \"mood\": \"happy\",\r\n  \"model-details\": {\r\n    \"initialized\": \"today\",\r\n    \"name\": \"gpt\"\r\n  }\r\n}\r\n```\r\n\r\nOpenAI, as much as admitted this when they said that ChatGPT-3.5 didn't pay enough attention to the system message (where you define much of what you want the model to respond like), which meant that the above solution was still hit-or-miss. The result of this was clear in how much extra research and development time developers would spend fine-tuning prompts and writing parsing code in order to get a response to return in a (semi) predictable fashion after they got the response they wanted.\r\n\r\nSolving this problem became crucial in allowing programmers to integrate these models into existing technologies: because ultimately software is built around defined and deterministic structures—even when the choices made inside the structures are anything but deterministic.\r\n\r\nFunction Calling: A Better Filter\r\nEnter function calling with the latest fine-tuning of the chat models. OpenAI did indeed greatly improve on the deficiency of the previous iterations, with the function playing the part of the system message: \"Under the hood, functions are injected into the system message in a syntax the model has been trained on.\" And, in truth, a better name for it may be a filter rather than a function because it is reducing the noise of the output and allows you to consistently and predictably extract or generate features from input data.\r\n\r\nFor example, say you have a large article and you want to find out the major themes, characters, dates, and events that are mentioned in it. Before, you could ask Chat to do that and you would get back paragraphs of text, or perhaps with a little prompt-engineering, come away with a couple lines of text. At worst, if the text was very long you might have to prompt it for each answer separately. Not bad, especially if it is a once-off job. But it becomes rather difficult to do this at scale—you have multiple potential breakpoints: maybe you were extracting words using regex and it returns it differently 30% of the time, or perhaps it returns it as a numbered list instead of bullet points. And let's not forget the costs of all this extraneous text…\r\n\r\nMoving towards Determinism\r\nBut now, machine readability is put to the fore—saving both time in development and cutting costs in production. You can define what you want returned in parameters—a name, a description (perhaps with an example) and a type—and, yes, as the docs admit, it may generate invalid JSON or hallucinate parameters, but at the very least you have a defined structure: the cornerstone of a programmable system.\r\n\r\nWhat is even more interesting is that a conversation can pass multiple functions (potential filters), and the model can choose which one to apply. You are only really limited by the context window. If one of them applies, it will return the stringified JSON answer that you specified in the function setup, which you can pass on to another API (like the weather API, or Amazon for shopping, or your own models, or into a database, etc.); and, if none apply, it will respond with a normal conversational response. As an aside, I am very curious as to what they are using to figure out what function to apply on an input text, but I feel that's another conversation…\r\n\r\nThis was the missing piece of the puzzle for developing with OpenAI's chat models in the wild beyond the myriad of implementations document summarization and chat bots towards more agentive systems—one's that communicate with each other or with themselves. I highly advise you to check out this video on [Function Calling](https://www.youtube.com/watch?v=0lOSvOoF2to) by sentdex for some fun examples and a nice notebook to play with.\r\n\r\nAlong with this feature being available for gpt4 also, increased context window size (16k for gpt-3.5!) and with gpt-3.5-turbo 25% cheaper, I expect to see the integration of LLMs accelerate across the tech space.\r\n\r\n```json\r\n{\r\n“Title”: “Functional Filtering with ChatGPT”\r\n“Summary”: “LLMs still going brrr thanks to timesaving with JSON outputs”,\r\n}\r\n```",
    "code": "var Component=(()=>{var lt=Object.create;var A=Object.defineProperty;var ut=Object.getOwnPropertyDescriptor;var ct=Object.getOwnPropertyNames;var dt=Object.getPrototypeOf,ft=Object.prototype.hasOwnProperty;var z=(c,a)=>()=>(a||c((a={exports:{}}).exports,a),a.exports),ht=(c,a)=>{for(var b in a)A(c,b,{get:a[b],enumerable:!0})},we=(c,a,b,y)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let _ of ct(a))!ft.call(c,_)&&_!==b&&A(c,_,{get:()=>a[_],enumerable:!(y=ut(a,_))||y.enumerable});return c};var mt=(c,a,b)=>(b=c!=null?lt(dt(c)):{},we(a||!c||!c.__esModule?A(b,\"default\",{value:c,enumerable:!0}):b,c)),pt=c=>we(A({},\"__esModule\",{value:!0}),c);var Ee=z((wt,xe)=>{xe.exports=React});var Ce=z(J=>{\"use strict\";(function(){\"use strict\";var c=Ee(),a=Symbol.for(\"react.element\"),b=Symbol.for(\"react.portal\"),y=Symbol.for(\"react.fragment\"),_=Symbol.for(\"react.strict_mode\"),q=Symbol.for(\"react.profiler\"),X=Symbol.for(\"react.provider\"),K=Symbol.for(\"react.context\"),R=Symbol.for(\"react.forward_ref\"),F=Symbol.for(\"react.suspense\"),I=Symbol.for(\"react.suspense_list\"),k=Symbol.for(\"react.memo\"),U=Symbol.for(\"react.lazy\"),ke=Symbol.for(\"react.offscreen\"),H=Symbol.iterator,Oe=\"@@iterator\";function Pe(e){if(e===null||typeof e!=\"object\")return null;var t=H&&e[H]||e[Oe];return typeof t==\"function\"?t:null}var x=c.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function h(e){{for(var t=arguments.length,n=new Array(t>1?t-1:0),r=1;r<t;r++)n[r-1]=arguments[r];De(\"error\",e,n)}}function De(e,t,n){{var r=x.ReactDebugCurrentFrame,s=r.getStackAddendum();s!==\"\"&&(t+=\"%s\",n=n.concat([s]));var l=n.map(function(i){return String(i)});l.unshift(\"Warning: \"+t),Function.prototype.apply.call(console[e],console,l)}}var Se=!1,je=!1,Ae=!1,Fe=!1,Ie=!1,Z;Z=Symbol.for(\"react.module.reference\");function Ue(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===y||e===q||Ie||e===_||e===F||e===I||Fe||e===ke||Se||je||Ae||typeof e==\"object\"&&e!==null&&(e.$$typeof===U||e.$$typeof===k||e.$$typeof===X||e.$$typeof===K||e.$$typeof===R||e.$$typeof===Z||e.getModuleId!==void 0))}function Me(e,t,n){var r=e.displayName;if(r)return r;var s=t.displayName||t.name||\"\";return s!==\"\"?n+\"(\"+s+\")\":n}function Q(e){return e.displayName||\"Context\"}function g(e){if(e==null)return null;if(typeof e.tag==\"number\"&&h(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case y:return\"Fragment\";case b:return\"Portal\";case q:return\"Profiler\";case _:return\"StrictMode\";case F:return\"Suspense\";case I:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case K:var t=e;return Q(t)+\".Consumer\";case X:var n=e;return Q(n._context)+\".Provider\";case R:return Me(e,e.render,\"ForwardRef\");case k:var r=e.displayName||null;return r!==null?r:g(e.type)||\"Memo\";case U:{var s=e,l=s._payload,i=s._init;try{return g(i(l))}catch{return null}}}return null}var w=Object.assign,N=0,ee,te,ne,re,ae,oe,ie;function se(){}se.__reactDisabledLog=!0;function Le(){{if(N===0){ee=console.log,te=console.info,ne=console.warn,re=console.error,ae=console.group,oe=console.groupCollapsed,ie=console.groupEnd;var e={configurable:!0,enumerable:!0,value:se,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}N++}}function Ye(){{if(N--,N===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:w({},e,{value:ee}),info:w({},e,{value:te}),warn:w({},e,{value:ne}),error:w({},e,{value:re}),group:w({},e,{value:ae}),groupCollapsed:w({},e,{value:oe}),groupEnd:w({},e,{value:ie})})}N<0&&h(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var M=x.ReactCurrentDispatcher,L;function O(e,t,n){{if(L===void 0)try{throw Error()}catch(s){var r=s.stack.trim().match(/\\n( *(at )?)/);L=r&&r[1]||\"\"}return`\n`+L+e}}var Y=!1,P;{var We=typeof WeakMap==\"function\"?WeakMap:Map;P=new We}function le(e,t){if(!e||Y)return\"\";{var n=P.get(e);if(n!==void 0)return n}var r;Y=!0;var s=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var l;l=M.current,M.current=null,Le();try{if(t){var i=function(){throw Error()};if(Object.defineProperty(i.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(i,[])}catch(v){r=v}Reflect.construct(e,[],i)}else{try{i.call()}catch(v){r=v}e.call(i.prototype)}}else{try{throw Error()}catch(v){r=v}e()}}catch(v){if(v&&r&&typeof v.stack==\"string\"){for(var o=v.stack.split(`\n`),m=r.stack.split(`\n`),d=o.length-1,f=m.length-1;d>=1&&f>=0&&o[d]!==m[f];)f--;for(;d>=1&&f>=0;d--,f--)if(o[d]!==m[f]){if(d!==1||f!==1)do if(d--,f--,f<0||o[d]!==m[f]){var p=`\n`+o[d].replace(\" at new \",\" at \");return e.displayName&&p.includes(\"<anonymous>\")&&(p=p.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&P.set(e,p),p}while(d>=1&&f>=0);break}}}finally{Y=!1,M.current=l,Ye(),Error.prepareStackTrace=s}var C=e?e.displayName||e.name:\"\",ye=C?O(C):\"\";return typeof e==\"function\"&&P.set(e,ye),ye}function $e(e,t,n){return le(e,!1)}function Ve(e){var t=e.prototype;return!!(t&&t.isReactComponent)}function D(e,t,n){if(e==null)return\"\";if(typeof e==\"function\")return le(e,Ve(e));if(typeof e==\"string\")return O(e);switch(e){case F:return O(\"Suspense\");case I:return O(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case R:return $e(e.render);case k:return D(e.type,t,n);case U:{var r=e,s=r._payload,l=r._init;try{return D(l(s),t,n)}catch{}}}return\"\"}var S=Object.prototype.hasOwnProperty,ue={},ce=x.ReactDebugCurrentFrame;function j(e){if(e){var t=e._owner,n=D(e.type,e._source,t?t.type:null);ce.setExtraStackFrame(n)}else ce.setExtraStackFrame(null)}function Be(e,t,n,r,s){{var l=Function.call.bind(S);for(var i in e)if(l(e,i)){var o=void 0;try{if(typeof e[i]!=\"function\"){var m=Error((r||\"React class\")+\": \"+n+\" type `\"+i+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[i]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw m.name=\"Invariant Violation\",m}o=e[i](t,i,r,n,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(d){o=d}o&&!(o instanceof Error)&&(j(s),h(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",r||\"React class\",n,i,typeof o),j(null)),o instanceof Error&&!(o.message in ue)&&(ue[o.message]=!0,j(s),h(\"Failed %s type: %s\",n,o.message),j(null))}}}var Ge=Array.isArray;function W(e){return Ge(e)}function ze(e){{var t=typeof Symbol==\"function\"&&Symbol.toStringTag,n=t&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return n}}function Je(e){try{return de(e),!1}catch{return!0}}function de(e){return\"\"+e}function fe(e){if(Je(e))return h(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",ze(e)),de(e)}var T=x.ReactCurrentOwner,qe={key:!0,ref:!0,__self:!0,__source:!0},he,me,$;$={};function Xe(e){if(S.call(e,\"ref\")){var t=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(t&&t.isReactWarning)return!1}return e.ref!==void 0}function Ke(e){if(S.call(e,\"key\")){var t=Object.getOwnPropertyDescriptor(e,\"key\").get;if(t&&t.isReactWarning)return!1}return e.key!==void 0}function He(e,t){if(typeof e.ref==\"string\"&&T.current&&t&&T.current.stateNode!==t){var n=g(T.current.type);$[n]||(h('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',g(T.current.type),e.ref),$[n]=!0)}}function Ze(e,t){{var n=function(){he||(he=!0,h(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",t))};n.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:n,configurable:!0})}}function Qe(e,t){{var n=function(){me||(me=!0,h(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",t))};n.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:n,configurable:!0})}}var et=function(e,t,n,r,s,l,i){var o={$$typeof:a,type:e,key:t,ref:n,props:i,_owner:l};return o._store={},Object.defineProperty(o._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:r}),Object.defineProperty(o,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:s}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function tt(e,t,n,r,s){{var l,i={},o=null,m=null;n!==void 0&&(fe(n),o=\"\"+n),Ke(t)&&(fe(t.key),o=\"\"+t.key),Xe(t)&&(m=t.ref,He(t,s));for(l in t)S.call(t,l)&&!qe.hasOwnProperty(l)&&(i[l]=t[l]);if(e&&e.defaultProps){var d=e.defaultProps;for(l in d)i[l]===void 0&&(i[l]=d[l])}if(o||m){var f=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;o&&Ze(i,f),m&&Qe(i,f)}return et(e,o,m,s,r,T.current,i)}}var V=x.ReactCurrentOwner,pe=x.ReactDebugCurrentFrame;function E(e){if(e){var t=e._owner,n=D(e.type,e._source,t?t.type:null);pe.setExtraStackFrame(n)}else pe.setExtraStackFrame(null)}var B;B=!1;function G(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===a}function be(){{if(V.current){var e=g(V.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function nt(e){{if(e!==void 0){var t=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),n=e.lineNumber;return`\n\nCheck your code at `+t+\":\"+n+\".\"}return\"\"}}var ge={};function rt(e){{var t=be();if(!t){var n=typeof e==\"string\"?e:e.displayName||e.name;n&&(t=`\n\nCheck the top-level render call using <`+n+\">.\")}return t}}function ve(e,t){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var n=rt(t);if(ge[n])return;ge[n]=!0;var r=\"\";e&&e._owner&&e._owner!==V.current&&(r=\" It was passed a child from \"+g(e._owner.type)+\".\"),E(e),h('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',n,r),E(null)}}function _e(e,t){{if(typeof e!=\"object\")return;if(W(e))for(var n=0;n<e.length;n++){var r=e[n];G(r)&&ve(r,t)}else if(G(e))e._store&&(e._store.validated=!0);else if(e){var s=Pe(e);if(typeof s==\"function\"&&s!==e.entries)for(var l=s.call(e),i;!(i=l.next()).done;)G(i.value)&&ve(i.value,t)}}}function at(e){{var t=e.type;if(t==null||typeof t==\"string\")return;var n;if(typeof t==\"function\")n=t.propTypes;else if(typeof t==\"object\"&&(t.$$typeof===R||t.$$typeof===k))n=t.propTypes;else return;if(n){var r=g(t);Be(n,e.props,\"prop\",r,e)}else if(t.PropTypes!==void 0&&!B){B=!0;var s=g(t);h(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",s||\"Unknown\")}typeof t.getDefaultProps==\"function\"&&!t.getDefaultProps.isReactClassApproved&&h(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function ot(e){{for(var t=Object.keys(e.props),n=0;n<t.length;n++){var r=t[n];if(r!==\"children\"&&r!==\"key\"){E(e),h(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",r),E(null);break}}e.ref!==null&&(E(e),h(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),E(null))}}function it(e,t,n,r,s,l){{var i=Ue(e);if(!i){var o=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(o+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var m=nt(s);m?o+=m:o+=be();var d;e===null?d=\"null\":W(e)?d=\"array\":e!==void 0&&e.$$typeof===a?(d=\"<\"+(g(e.type)||\"Unknown\")+\" />\",o=\" Did you accidentally export a JSX literal instead of a component?\"):d=typeof e,h(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",d,o)}var f=tt(e,t,n,s,l);if(f==null)return f;if(i){var p=t.children;if(p!==void 0)if(r)if(W(p)){for(var C=0;C<p.length;C++)_e(p[C],e);Object.freeze&&Object.freeze(p)}else h(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else _e(p,e)}return e===y?ot(f):at(f),f}}var st=it;J.Fragment=y,J.jsxDEV=st})()});var Te=z((Et,Ne)=>{\"use strict\";Ne.exports=Ce()});var _t={};ht(_t,{default:()=>vt,frontmatter:()=>bt});var u=mt(Te()),bt={title:\"Functional Filtering with ChatGPT\",description:\"LLMs still going brrr thanks to timesaving with JSON outputs\",updatedAt:\"2023-06-16\",publishedAt:\"2023-06-16\",author:\"Ois\\xEDn Thomas Morrin\",image:\"../../public/profile.png\",majorTag:\"Thoughts\",subTag:\"Tech\",language:\"en\",tags:[\"Tech\",\"ChatGPT\",\"OpenAI\",\"AI\",null]};function Re(c){let a=Object.assign({p:\"p\",a:\"a\",h2:\"h2\",pre:\"pre\",code:\"code\"},c.components);return(0,u.jsxDEV)(u.Fragment,{children:[(0,u.jsxDEV)(a.p,{children:[\"Another week, another slew of AI updates. In particular, this week has seen a very exciting new development from OpenAI: function calling. Their post about it was \",(0,u.jsxDEV)(a.a,{href:\"https://platform.openai.com/docs/guides/gpt/function-calling\",children:\"rather uninspiring\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:19,columnNumber:164},this),\", so I wanted to give it a closer look and show you why it came about, why it matters, and how it is really useful.\"]},void 0,!0,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:19,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.h2,{children:\"Up to Now\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:21,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:`Chat completions using OpenAI have taken the world by storm, with \"Ask GPT\" trending on Twitter and much fear and excitement for what LLMs will mean for the world of work\\u2014especially in the realm of content creation. And, that isn't really surprising because, up to now, the most obvious use cases for the chat models have been that: conversation and content media creation.`},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:23,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:\"The reach of the technology stepped forward with the advent of ChatGPT plugins\\u2014allowing it to access the web from within the scope of the chat window for the first time, and not just through the developer's API.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:25,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.h2,{children:\"The Information to Noise Ratio\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:27,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:\"But, a large problem remained: it is very difficult to extract information from a conversation and these Chat models were rather fallible when it came to extracting key pieces of information from text in a predictable way.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:29,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:\"An early fix for this was asking the model to respond in JSON format which is common across the internet itself, basically key-value pairs. For example:\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:31,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.pre,{children:(0,u.jsxDEV)(a.code,{className:\"language-json\",children:`{\\r\n  \"mood\": \"happy\",\\r\n  \"model-details\": {\\r\n    \"initialized\": \"today\",\\r\n    \"name\": \"gpt\"\\r\n  }\\r\n}\n`},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:33,columnNumber:1},this)},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:33,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:\"OpenAI, as much as admitted this when they said that ChatGPT-3.5 didn't pay enough attention to the system message (where you define much of what you want the model to respond like), which meant that the above solution was still hit-or-miss. The result of this was clear in how much extra research and development time developers would spend fine-tuning prompts and writing parsing code in order to get a response to return in a (semi) predictable fashion after they got the response they wanted.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:43,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:\"Solving this problem became crucial in allowing programmers to integrate these models into existing technologies: because ultimately software is built around defined and deterministic structures\\u2014even when the choices made inside the structures are anything but deterministic.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:45,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:`Function Calling: A Better Filter\\r\nEnter function calling with the latest fine-tuning of the chat models. OpenAI did indeed greatly improve on the deficiency of the previous iterations, with the function playing the part of the system message: \"Under the hood, functions are injected into the system message in a syntax the model has been trained on.\" And, in truth, a better name for it may be a filter rather than a function because it is reducing the noise of the output and allows you to consistently and predictably extract or generate features from input data.`},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:47,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:\"For example, say you have a large article and you want to find out the major themes, characters, dates, and events that are mentioned in it. Before, you could ask Chat to do that and you would get back paragraphs of text, or perhaps with a little prompt-engineering, come away with a couple lines of text. At worst, if the text was very long you might have to prompt it for each answer separately. Not bad, especially if it is a once-off job. But it becomes rather difficult to do this at scale\\u2014you have multiple potential breakpoints: maybe you were extracting words using regex and it returns it differently 30% of the time, or perhaps it returns it as a numbered list instead of bullet points. And let's not forget the costs of all this extraneous text\\u2026\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:50,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:`Moving towards Determinism\\r\nBut now, machine readability is put to the fore\\u2014saving both time in development and cutting costs in production. You can define what you want returned in parameters\\u2014a name, a description (perhaps with an example) and a type\\u2014and, yes, as the docs admit, it may generate invalid JSON or hallucinate parameters, but at the very least you have a defined structure: the cornerstone of a programmable system.`},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:52,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:\"What is even more interesting is that a conversation can pass multiple functions (potential filters), and the model can choose which one to apply. You are only really limited by the context window. If one of them applies, it will return the stringified JSON answer that you specified in the function setup, which you can pass on to another API (like the weather API, or Amazon for shopping, or your own models, or into a database, etc.); and, if none apply, it will respond with a normal conversational response. As an aside, I am very curious as to what they are using to figure out what function to apply on an input text, but I feel that's another conversation\\u2026\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:55,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:[\"This was the missing piece of the puzzle for developing with OpenAI's chat models in the wild beyond the myriad of implementations document summarization and chat bots towards more agentive systems\\u2014one's that communicate with each other or with themselves. I highly advise you to check out this video on \",(0,u.jsxDEV)(a.a,{href:\"https://www.youtube.com/watch?v=0lOSvOoF2to\",children:\"Function Calling\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:57,columnNumber:305},this),\" by sentdex for some fun examples and a nice notebook to play with.\"]},void 0,!0,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:57,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.p,{children:\"Along with this feature being available for gpt4 also, increased context window size (16k for gpt-3.5!) and with gpt-3.5-turbo 25% cheaper, I expect to see the integration of LLMs accelerate across the tech space.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:59,columnNumber:1},this),`\n`,(0,u.jsxDEV)(a.pre,{children:(0,u.jsxDEV)(a.code,{className:\"language-json\",children:`{\\r\n\\u201CTitle\\u201D: \\u201CFunctional Filtering with ChatGPT\\u201D\\r\n\\u201CSummary\\u201D: \\u201CLLMs still going brrr thanks to timesaving with JSON outputs\\u201D,\\r\n}\n`},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:61,columnNumber:1},this)},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:61,columnNumber:1},this)]},void 0,!0,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\",lineNumber:1,columnNumber:1},this)}function gt(c={}){let{wrapper:a}=c.components||{};return a?(0,u.jsxDEV)(a,Object.assign({},c,{children:(0,u.jsxDEV)(Re,c,void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\"},this)}),void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-841c5057-3cda-4831-9d6d-6cdfa102632c.mdx\"},this):Re(c)}var vt=gt;return pt(_t);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "functional-filtering-with-chatgpt/index.mdx",
  "_raw": {
    "sourceFilePath": "functional-filtering-with-chatgpt/index.mdx",
    "sourceFileName": "index.mdx",
    "sourceFileDir": "functional-filtering-with-chatgpt",
    "contentType": "mdx",
    "flattenedPath": "functional-filtering-with-chatgpt"
  },
  "type": "Blog",
  "url": "/blog/functional-filtering-with-chatgpt",
  "readingTime": {
    "text": "5 min read",
    "minutes": 4.495,
    "time": 269700,
    "words": 899
  },
  "toc": [
    {
      "level": "two",
      "text": "Up to Now",
      "slug": "up-to-now"
    },
    {
      "level": "two",
      "text": "The Information to Noise Ratio",
      "slug": "the-information-to-noise-ratio"
    }
  ]
}