{
  "title": "Anyone in Need of Assistants?",
  "publishedAt": "2023-11-07T00:00:00.000Z",
  "updatedAt": "2023-11-07T00:00:00.000Z",
  "description": "DevDay for OpenAI brings AI agents to the masses",
  "image": {
    "filePath": "../public/profile.png",
    "relativeFilePath": "../../public/profile.png",
    "format": "png",
    "height": 1024,
    "width": 1024,
    "aspectRatio": 1,
    "blurhashDataUrl": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAYAAADED76LAAAA+UlEQVR4nDXOP0gCYRjH8be5hpaCqCUCawjiaqjBtSUInAoiqK2xvUEcWyIIClzaioY0EbzT/k0ph0UnSRiXkYKciHKTnoP6fn3h8APPAz+e3/CInm1Qz95Sf0/ilnJ8vT6QNeKkkvekHnUETgHv38SxnnD0C7anxwmtBCiaH+TNHKJbtcC1qaUvyUT2SEcO2NnQWJiaoVT8RUj3j4oRxQjNMTsm2F1fJTA5gTavsbS8hRh4TaxEFOhi59+Q7TInm2sEF4M0Wh31g/J8c6W20vnBc74p6zFOj8+RUvoFkGp8n/E7dehzdp1RiVHB9xLep5CIcXQYVsk3BHqjyD2wCBcLAAAAK3RFWHRDcmVhdGlvbiBUaW1lAFRodSwgMTYgTm92IDIwMjMgMTA6MTY6NTkgR01UgqJG6gAAAC50RVh0U29mdHdhcmUAZ2l0aHViLmNvbS9tYXRtZW4vSW1hZ2VTY3JpcHQgdjEuMi4xNujZvYYAAAAASUVORK5CYII="
  },
  "isPublished": true,
  "author": "OisÃ­n Thomas Morrin",
  "tags": [
    "Tech",
    "ChatGPT",
    "OpenAI",
    "AI",
    "\r"
  ],
  "majorTag": "Tech",
  "body": {
    "raw": "\r\nOpenAI have had a year to remember, with GPT-3, GPT4, vision, text-to-speech, automated speech... and surely they couldn't have more to offer other than minor tweaks of pricing and performance, right? But, they do; and, in many ways I think the latest developments released on their DevDay today are probably the most impactful so far.\r\n\r\nIf you are impatient like me and want to know the most important changes, skip past the developments and changes and head straight to the GPTs and API Assistants.\r\n\r\n### Developments & Changes\r\nThere are days' worth of developments and breaking changes:\r\n - Context length can now reach up to 128k context for GPT-4 turbo (yes, a faster GPT-4 model version!), which is approximately 300 pages of a standard book in English. Additionally, they have made model improvements on attention over long context leading to improved accuracy. I think we will have to explore this a lot over the coming weeks to see how tangible this improvement in accuracy in.\r\n - Last time I wrote about [function calls](https://www.koukishinousei.com/blog/functional-filtering-with-chatgpt) and the power of determinism in outputs of the model. OpenAI have pushed that a step forward now and now offer JSON formatted responses as an option, rather than a hack. Speaking of function calls, we can now do the thing we wanted to do: be able to call multiple functions at once (looking forward to trying this out for the order of functions).\r\n - Something you may have noticed particularly with DALLE-3 is that you can now get images to be consistent somewhat more often, allowing you to persist characters across frames. This as is noted in a [Simon Willison blog post](https://simonwillison.net/2023/Oct/26/add-a-walrus/)) is thanks to the seed (thing of as a random number id) that is given to a prompt, which will now allow for more deterministic and reproducible outputs for both images and conversations. I imagine this will help constrain the randomness that higher temperatures can bring to the table, without having to lose the novelty of the chat interaction by reducing it to less than 0.1. Another change that has happened recently is the ability to upload multiple images to GPT-with-vision at once (and not just for the chat interface but for the API too) which will allow for easier prompting for synthesis and extraction of information from images.\r\n - Another thing announced, but in this case not released yet, are log probs in the API... for those of you don't know, this can be seen as the probability (its log) of how likely it is to come next. It is super useful for debugging the behaviour of your model as you try to get it to produce certain outputs.\r\n - The onerous issue of the knowledge cut-off being September 2021 has finally been brought forward: for GPT-4 Turbo it is April 2023.\r\n - If this wasn't enough, we are entering the stage of GPT's having the ability to interact with the world through new modalities -- and not just from the chat interface. The new Text-to-speech model (which comes with 6 natural sounding voices), dalle-3 and GPT-4 with vision are all now available from the API which is super exciting to see. One thing I am particularly interested in is now being able to upload multiple images at once to GPT-with-vision. Along with text-to-speech being fully integrated, Whisper V3 (the automated speech recognition model) is now available, and will soon be part of the API also.\r\n - It is all about the fine-details, and customisability is key with GPT. Fine-tuning is being rolled out for GPT-4. In addition to this, for bigger companies with lots of proprietary data, they will be rolling out custom models which will give companies a close partnership with OpenAI to build tools to suit their needs.\r\n - And then the expected one: rate limit increases -- 2x tokens per minute for all models with control over rate limits from the platform console. They also have developed a tiered system for the rate limits meaning heavier users can more generous rate limits. Additionally, pricing is another strength of the GPT-series, which will continue to look to be the cheapest-in-class way to use high-performance large language models. They make sure of this again by reducing GPT-4 Turbo costs by x3 for inputs and x2 for outputs, and also doing the same for the 16k version of GPT-3.5 Turbo.\r\n - Interestingly, the mention that improving latency is next on their agenda...\r\n\r\nSome of the subtle announcements but big commitments is their commitment through \"Copyright Shield\" to protecting customers who face legal claims on copyright infringement for those who use ChatGPT Enterprise or the API. This is a massive commitment from the company in the wake of the magnitude of the outcry from professionals from all fields talking about breaking data privacy laws and copyright infringement through the data the model is trained on.\r\n\r\nAnother subtle change is the removal of the drop-down menu for choosing what plugins to use. They model will now use function calling itself to decide when to use which plugin.\r\n\r\n\"Everyone gets a GPT\" is the zeitgeist of the latter segment of the Opening Keynote with two massive features:\r\n\r\n### GPTS\r\nGPTs are tailored versions of ChatGPT for a specific purpose, with instructions, expanded knowledge (think retrieval) and actions (think function calls). And this isn't just for the technically literate, these can be programmed with natural language alone. These can all be run from the ChatGPT interface itself, allowing you to design, plan and play with various APIs and products (like Canva or anything used in Zapier). \r\n\r\nAnd what's more, they are creating a marketplace with revenue sharing as an incentive for popular GPTs that are shared by users. It will be great to see a place where models can be shared, because up to now, it has felt so restrictive: you had to always build your own, or you ended up in some 3rd party marketplace for prompts etc. This will likely centralise the LLM-community in a way that hasn't been done before.\r\n\r\n### Assistants API\r\nThese are the first real step by OpenAI into agentive AI. Honestly, they make massive inroads into the development of them, by clearing up issues with state management by using threads to remember conversations, extended capabilities which now allow code-interpreter and function-calling and built-in retrieval -- this should solve the problems we had for context windows and chunking.\r\n\r\nFor anyone who has tried to build an agent in the past, they know it has been tough and required a lot of boutique solutions. Now many of the core backend problems have been removed or greatly reduced in a similar way to how the front-end development and middle-ware was improved by Vercel's AI SDK. Prototyping should become much easier and more rapid than ever with these changes, with a very nifty UI. \r\n\r\nOne thing to note is that the maximum number of files that you can use as an extended knowledge base or \"retrieval files\" is 20. And, currently, it is down (couldn't help but prep a project as soon as it launched).\r\n",
    "code": "var Component=(()=>{var un=Object.create;var I=Object.defineProperty;var cn=Object.getOwnPropertyDescriptor;var fn=Object.getOwnPropertyNames;var dn=Object.getPrototypeOf,bn=Object.prototype.hasOwnProperty;var q=(c,r)=>()=>(r||c((r={exports:{}}).exports,r),r.exports),hn=(c,r)=>{for(var p in r)I(c,p,{get:r[p],enumerable:!0})},we=(c,r,p,y)=>{if(r&&typeof r==\"object\"||typeof r==\"function\")for(let _ of fn(r))!bn.call(c,_)&&_!==p&&I(c,_,{get:()=>r[_],enumerable:!(y=cn(r,_))||y.enumerable});return c};var mn=(c,r,p)=>(p=c!=null?un(dn(c)):{},we(r||!c||!c.__esModule?I(p,\"default\",{value:c,enumerable:!0}):p,c)),pn=c=>we(I({},\"__esModule\",{value:!0}),c);var Te=q((xn,xe)=>{xe.exports=React});var Ee=q(z=>{\"use strict\";(function(){\"use strict\";var c=Te(),r=Symbol.for(\"react.element\"),p=Symbol.for(\"react.portal\"),y=Symbol.for(\"react.fragment\"),_=Symbol.for(\"react.strict_mode\"),K=Symbol.for(\"react.profiler\"),X=Symbol.for(\"react.provider\"),H=Symbol.for(\"react.context\"),C=Symbol.for(\"react.forward_ref\"),j=Symbol.for(\"react.suspense\"),U=Symbol.for(\"react.suspense_list\"),P=Symbol.for(\"react.memo\"),F=Symbol.for(\"react.lazy\"),Pe=Symbol.for(\"react.offscreen\"),J=Symbol.iterator,Re=\"@@iterator\";function De(e){if(e===null||typeof e!=\"object\")return null;var n=J&&e[J]||e[Re];return typeof n==\"function\"?n:null}var x=c.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function b(e){{for(var n=arguments.length,t=new Array(n>1?n-1:0),o=1;o<n;o++)t[o-1]=arguments[o];Oe(\"error\",e,t)}}function Oe(e,n,t){{var o=x.ReactDebugCurrentFrame,l=o.getStackAddendum();l!==\"\"&&(n+=\"%s\",t=t.concat([l]));var u=t.map(function(a){return String(a)});u.unshift(\"Warning: \"+n),Function.prototype.apply.call(console[e],console,u)}}var Ae=!1,Se=!1,Ie=!1,je=!1,Ue=!1,Z;Z=Symbol.for(\"react.module.reference\");function Fe(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===y||e===K||Ue||e===_||e===j||e===U||je||e===Pe||Ae||Se||Ie||typeof e==\"object\"&&e!==null&&(e.$$typeof===F||e.$$typeof===P||e.$$typeof===X||e.$$typeof===H||e.$$typeof===C||e.$$typeof===Z||e.getModuleId!==void 0))}function Ge(e,n,t){var o=e.displayName;if(o)return o;var l=n.displayName||n.name||\"\";return l!==\"\"?t+\"(\"+l+\")\":t}function Q(e){return e.displayName||\"Context\"}function g(e){if(e==null)return null;if(typeof e.tag==\"number\"&&b(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case y:return\"Fragment\";case p:return\"Portal\";case K:return\"Profiler\";case _:return\"StrictMode\";case j:return\"Suspense\";case U:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case H:var n=e;return Q(n)+\".Consumer\";case X:var t=e;return Q(t._context)+\".Provider\";case C:return Ge(e,e.render,\"ForwardRef\");case P:var o=e.displayName||null;return o!==null?o:g(e.type)||\"Memo\";case F:{var l=e,u=l._payload,a=l._init;try{return g(a(u))}catch{return null}}}return null}var w=Object.assign,k=0,ee,ne,te,re,oe,ie,ae;function se(){}se.__reactDisabledLog=!0;function We(){{if(k===0){ee=console.log,ne=console.info,te=console.warn,re=console.error,oe=console.group,ie=console.groupCollapsed,ae=console.groupEnd;var e={configurable:!0,enumerable:!0,value:se,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}k++}}function Ye(){{if(k--,k===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:w({},e,{value:ee}),info:w({},e,{value:ne}),warn:w({},e,{value:te}),error:w({},e,{value:re}),group:w({},e,{value:oe}),groupCollapsed:w({},e,{value:ie}),groupEnd:w({},e,{value:ae})})}k<0&&b(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var G=x.ReactCurrentDispatcher,W;function R(e,n,t){{if(W===void 0)try{throw Error()}catch(l){var o=l.stack.trim().match(/\\n( *(at )?)/);W=o&&o[1]||\"\"}return`\n`+W+e}}var Y=!1,D;{var $e=typeof WeakMap==\"function\"?WeakMap:Map;D=new $e}function le(e,n){if(!e||Y)return\"\";{var t=D.get(e);if(t!==void 0)return t}var o;Y=!0;var l=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var u;u=G.current,G.current=null,We();try{if(n){var a=function(){throw Error()};if(Object.defineProperty(a.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(a,[])}catch(v){o=v}Reflect.construct(e,[],a)}else{try{a.call()}catch(v){o=v}e.call(a.prototype)}}else{try{throw Error()}catch(v){o=v}e()}}catch(v){if(v&&o&&typeof v.stack==\"string\"){for(var i=v.stack.split(`\n`),h=o.stack.split(`\n`),f=i.length-1,d=h.length-1;f>=1&&d>=0&&i[f]!==h[d];)d--;for(;f>=1&&d>=0;f--,d--)if(i[f]!==h[d]){if(f!==1||d!==1)do if(f--,d--,d<0||i[f]!==h[d]){var m=`\n`+i[f].replace(\" at new \",\" at \");return e.displayName&&m.includes(\"<anonymous>\")&&(m=m.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&D.set(e,m),m}while(f>=1&&d>=0);break}}}finally{Y=!1,G.current=u,Ye(),Error.prepareStackTrace=l}var E=e?e.displayName||e.name:\"\",ye=E?R(E):\"\";return typeof e==\"function\"&&D.set(e,ye),ye}function Le(e,n,t){return le(e,!1)}function Me(e){var n=e.prototype;return!!(n&&n.isReactComponent)}function O(e,n,t){if(e==null)return\"\";if(typeof e==\"function\")return le(e,Me(e));if(typeof e==\"string\")return R(e);switch(e){case j:return R(\"Suspense\");case U:return R(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case C:return Le(e.render);case P:return O(e.type,n,t);case F:{var o=e,l=o._payload,u=o._init;try{return O(u(l),n,t)}catch{}}}return\"\"}var A=Object.prototype.hasOwnProperty,ue={},ce=x.ReactDebugCurrentFrame;function S(e){if(e){var n=e._owner,t=O(e.type,e._source,n?n.type:null);ce.setExtraStackFrame(t)}else ce.setExtraStackFrame(null)}function Ve(e,n,t,o,l){{var u=Function.call.bind(A);for(var a in e)if(u(e,a)){var i=void 0;try{if(typeof e[a]!=\"function\"){var h=Error((o||\"React class\")+\": \"+t+\" type `\"+a+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[a]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw h.name=\"Invariant Violation\",h}i=e[a](n,a,o,t,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(f){i=f}i&&!(i instanceof Error)&&(S(l),b(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",o||\"React class\",t,a,typeof i),S(null)),i instanceof Error&&!(i.message in ue)&&(ue[i.message]=!0,S(l),b(\"Failed %s type: %s\",t,i.message),S(null))}}}var Be=Array.isArray;function $(e){return Be(e)}function qe(e){{var n=typeof Symbol==\"function\"&&Symbol.toStringTag,t=n&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return t}}function ze(e){try{return fe(e),!1}catch{return!0}}function fe(e){return\"\"+e}function de(e){if(ze(e))return b(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",qe(e)),fe(e)}var N=x.ReactCurrentOwner,Ke={key:!0,ref:!0,__self:!0,__source:!0},be,he,L;L={};function Xe(e){if(A.call(e,\"ref\")){var n=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(n&&n.isReactWarning)return!1}return e.ref!==void 0}function He(e){if(A.call(e,\"key\")){var n=Object.getOwnPropertyDescriptor(e,\"key\").get;if(n&&n.isReactWarning)return!1}return e.key!==void 0}function Je(e,n){if(typeof e.ref==\"string\"&&N.current&&n&&N.current.stateNode!==n){var t=g(N.current.type);L[t]||(b('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',g(N.current.type),e.ref),L[t]=!0)}}function Ze(e,n){{var t=function(){be||(be=!0,b(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",n))};t.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:t,configurable:!0})}}function Qe(e,n){{var t=function(){he||(he=!0,b(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",n))};t.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:t,configurable:!0})}}var en=function(e,n,t,o,l,u,a){var i={$$typeof:r,type:e,key:n,ref:t,props:a,_owner:u};return i._store={},Object.defineProperty(i._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(i,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:o}),Object.defineProperty(i,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:l}),Object.freeze&&(Object.freeze(i.props),Object.freeze(i)),i};function nn(e,n,t,o,l){{var u,a={},i=null,h=null;t!==void 0&&(de(t),i=\"\"+t),He(n)&&(de(n.key),i=\"\"+n.key),Xe(n)&&(h=n.ref,Je(n,l));for(u in n)A.call(n,u)&&!Ke.hasOwnProperty(u)&&(a[u]=n[u]);if(e&&e.defaultProps){var f=e.defaultProps;for(u in f)a[u]===void 0&&(a[u]=f[u])}if(i||h){var d=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;i&&Ze(a,d),h&&Qe(a,d)}return en(e,i,h,l,o,N.current,a)}}var M=x.ReactCurrentOwner,me=x.ReactDebugCurrentFrame;function T(e){if(e){var n=e._owner,t=O(e.type,e._source,n?n.type:null);me.setExtraStackFrame(t)}else me.setExtraStackFrame(null)}var V;V=!1;function B(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===r}function pe(){{if(M.current){var e=g(M.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function tn(e){{if(e!==void 0){var n=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),t=e.lineNumber;return`\n\nCheck your code at `+n+\":\"+t+\".\"}return\"\"}}var ge={};function rn(e){{var n=pe();if(!n){var t=typeof e==\"string\"?e:e.displayName||e.name;t&&(n=`\n\nCheck the top-level render call using <`+t+\">.\")}return n}}function ve(e,n){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var t=rn(n);if(ge[t])return;ge[t]=!0;var o=\"\";e&&e._owner&&e._owner!==M.current&&(o=\" It was passed a child from \"+g(e._owner.type)+\".\"),T(e),b('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',t,o),T(null)}}function _e(e,n){{if(typeof e!=\"object\")return;if($(e))for(var t=0;t<e.length;t++){var o=e[t];B(o)&&ve(o,n)}else if(B(e))e._store&&(e._store.validated=!0);else if(e){var l=De(e);if(typeof l==\"function\"&&l!==e.entries)for(var u=l.call(e),a;!(a=u.next()).done;)B(a.value)&&ve(a.value,n)}}}function on(e){{var n=e.type;if(n==null||typeof n==\"string\")return;var t;if(typeof n==\"function\")t=n.propTypes;else if(typeof n==\"object\"&&(n.$$typeof===C||n.$$typeof===P))t=n.propTypes;else return;if(t){var o=g(n);Ve(t,e.props,\"prop\",o,e)}else if(n.PropTypes!==void 0&&!V){V=!0;var l=g(n);b(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",l||\"Unknown\")}typeof n.getDefaultProps==\"function\"&&!n.getDefaultProps.isReactClassApproved&&b(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function an(e){{for(var n=Object.keys(e.props),t=0;t<n.length;t++){var o=n[t];if(o!==\"children\"&&o!==\"key\"){T(e),b(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",o),T(null);break}}e.ref!==null&&(T(e),b(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),T(null))}}function sn(e,n,t,o,l,u){{var a=Fe(e);if(!a){var i=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(i+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var h=tn(l);h?i+=h:i+=pe();var f;e===null?f=\"null\":$(e)?f=\"array\":e!==void 0&&e.$$typeof===r?(f=\"<\"+(g(e.type)||\"Unknown\")+\" />\",i=\" Did you accidentally export a JSX literal instead of a component?\"):f=typeof e,b(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",f,i)}var d=nn(e,n,t,l,u);if(d==null)return d;if(a){var m=n.children;if(m!==void 0)if(o)if($(m)){for(var E=0;E<m.length;E++)_e(m[E],e);Object.freeze&&Object.freeze(m)}else b(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else _e(m,e)}return e===y?an(d):on(d),d}}var ln=sn;z.Fragment=y,z.jsxDEV=ln})()});var Ne=q((En,ke)=>{\"use strict\";ke.exports=Ee()});var yn={};hn(yn,{default:()=>_n,frontmatter:()=>gn});var s=mn(Ne()),gn={title:\"Anyone in Need of Assistants?\",description:\"DevDay for OpenAI brings AI agents to the masses\",updatedAt:\"2023-11-07\",publishedAt:\"2023-11-07\",author:\"Ois\\xEDn Thomas Morrin\",image:\"../../public/profile.png\",majorTag:\"Tech\",tags:[\"Tech\",\"ChatGPT\",\"OpenAI\",\"AI\",null]};function Ce(c){let r=Object.assign({p:\"p\",h3:\"h3\",ul:\"ul\",li:\"li\",a:\"a\"},c.components);return(0,s.jsxDEV)(s.Fragment,{children:[(0,s.jsxDEV)(r.p,{children:\"OpenAI have had a year to remember, with GPT-3, GPT4, vision, text-to-speech, automated speech... and surely they couldn't have more to offer other than minor tweaks of pricing and performance, right? But, they do; and, in many ways I think the latest developments released on their DevDay today are probably the most impactful so far.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:17,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.p,{children:\"If you are impatient like me and want to know the most important changes, skip past the developments and changes and head straight to the GPTs and API Assistants.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:19,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.h3,{children:\"Developments & Changes\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:21,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.p,{children:\"There are days' worth of developments and breaking changes:\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:22,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.ul,{children:[`\n`,(0,s.jsxDEV)(r.li,{children:\"Context length can now reach up to 128k context for GPT-4 turbo (yes, a faster GPT-4 model version!), which is approximately 300 pages of a standard book in English. Additionally, they have made model improvements on attention over long context leading to improved accuracy. I think we will have to explore this a lot over the coming weeks to see how tangible this improvement in accuracy in.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:23,columnNumber:2},this),`\n`,(0,s.jsxDEV)(r.li,{children:[\"Last time I wrote about \",(0,s.jsxDEV)(r.a,{href:\"https://www.koukishinousei.com/blog/functional-filtering-with-chatgpt\",children:\"function calls\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:24,columnNumber:28},this),\" and the power of determinism in outputs of the model. OpenAI have pushed that a step forward now and now offer JSON formatted responses as an option, rather than a hack. Speaking of function calls, we can now do the thing we wanted to do: be able to call multiple functions at once (looking forward to trying this out for the order of functions).\"]},void 0,!0,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:24,columnNumber:2},this),`\n`,(0,s.jsxDEV)(r.li,{children:[\"Something you may have noticed particularly with DALLE-3 is that you can now get images to be consistent somewhat more often, allowing you to persist characters across frames. This as is noted in a \",(0,s.jsxDEV)(r.a,{href:\"https://simonwillison.net/2023/Oct/26/add-a-walrus/\",children:\"Simon Willison blog post\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:25,columnNumber:202},this),\") is thanks to the seed (thing of as a random number id) that is given to a prompt, which will now allow for more deterministic and reproducible outputs for both images and conversations. I imagine this will help constrain the randomness that higher temperatures can bring to the table, without having to lose the novelty of the chat interaction by reducing it to less than 0.1. Another change that has happened recently is the ability to upload multiple images to GPT-with-vision at once (and not just for the chat interface but for the API too) which will allow for easier prompting for synthesis and extraction of information from images.\"]},void 0,!0,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:25,columnNumber:2},this),`\n`,(0,s.jsxDEV)(r.li,{children:\"Another thing announced, but in this case not released yet, are log probs in the API... for those of you don't know, this can be seen as the probability (its log) of how likely it is to come next. It is super useful for debugging the behaviour of your model as you try to get it to produce certain outputs.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:26,columnNumber:2},this),`\n`,(0,s.jsxDEV)(r.li,{children:\"The onerous issue of the knowledge cut-off being September 2021 has finally been brought forward: for GPT-4 Turbo it is April 2023.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:27,columnNumber:2},this),`\n`,(0,s.jsxDEV)(r.li,{children:\"If this wasn't enough, we are entering the stage of GPT's having the ability to interact with the world through new modalities -- and not just from the chat interface. The new Text-to-speech model (which comes with 6 natural sounding voices), dalle-3 and GPT-4 with vision are all now available from the API which is super exciting to see. One thing I am particularly interested in is now being able to upload multiple images at once to GPT-with-vision. Along with text-to-speech being fully integrated, Whisper V3 (the automated speech recognition model) is now available, and will soon be part of the API also.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:28,columnNumber:2},this),`\n`,(0,s.jsxDEV)(r.li,{children:\"It is all about the fine-details, and customisability is key with GPT. Fine-tuning is being rolled out for GPT-4. In addition to this, for bigger companies with lots of proprietary data, they will be rolling out custom models which will give companies a close partnership with OpenAI to build tools to suit their needs.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:29,columnNumber:2},this),`\n`,(0,s.jsxDEV)(r.li,{children:\"And then the expected one: rate limit increases -- 2x tokens per minute for all models with control over rate limits from the platform console. They also have developed a tiered system for the rate limits meaning heavier users can more generous rate limits. Additionally, pricing is another strength of the GPT-series, which will continue to look to be the cheapest-in-class way to use high-performance large language models. They make sure of this again by reducing GPT-4 Turbo costs by x3 for inputs and x2 for outputs, and also doing the same for the 16k version of GPT-3.5 Turbo.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:30,columnNumber:2},this),`\n`,(0,s.jsxDEV)(r.li,{children:\"Interestingly, the mention that improving latency is next on their agenda...\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:31,columnNumber:2},this),`\n`]},void 0,!0,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:23,columnNumber:2},this),`\n`,(0,s.jsxDEV)(r.p,{children:'Some of the subtle announcements but big commitments is their commitment through \"Copyright Shield\" to protecting customers who face legal claims on copyright infringement for those who use ChatGPT Enterprise or the API. This is a massive commitment from the company in the wake of the magnitude of the outcry from professionals from all fields talking about breaking data privacy laws and copyright infringement through the data the model is trained on.'},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:33,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.p,{children:\"Another subtle change is the removal of the drop-down menu for choosing what plugins to use. They model will now use function calling itself to decide when to use which plugin.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:35,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.p,{children:'\"Everyone gets a GPT\" is the zeitgeist of the latter segment of the Opening Keynote with two massive features:'},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:37,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.h3,{children:\"GPTS\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:39,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.p,{children:\"GPTs are tailored versions of ChatGPT for a specific purpose, with instructions, expanded knowledge (think retrieval) and actions (think function calls). And this isn't just for the technically literate, these can be programmed with natural language alone. These can all be run from the ChatGPT interface itself, allowing you to design, plan and play with various APIs and products (like Canva or anything used in Zapier).\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:40,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.p,{children:\"And what's more, they are creating a marketplace with revenue sharing as an incentive for popular GPTs that are shared by users. It will be great to see a place where models can be shared, because up to now, it has felt so restrictive: you had to always build your own, or you ended up in some 3rd party marketplace for prompts etc. This will likely centralise the LLM-community in a way that hasn't been done before.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:42,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.h3,{children:\"Assistants API\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:44,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.p,{children:\"These are the first real step by OpenAI into agentive AI. Honestly, they make massive inroads into the development of them, by clearing up issues with state management by using threads to remember conversations, extended capabilities which now allow code-interpreter and function-calling and built-in retrieval -- this should solve the problems we had for context windows and chunking.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:45,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.p,{children:\"For anyone who has tried to build an agent in the past, they know it has been tough and required a lot of boutique solutions. Now many of the core backend problems have been removed or greatly reduced in a similar way to how the front-end development and middle-ware was improved by Vercel's AI SDK. Prototyping should become much easier and more rapid than ever with these changes, with a very nifty UI.\"},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:47,columnNumber:1},this),`\n`,(0,s.jsxDEV)(r.p,{children:`One thing to note is that the maximum number of files that you can use as an extended knowledge base or \"retrieval files\" is 20. And, currently, it is down (couldn't help but prep a project as soon as it launched).`},void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:49,columnNumber:1},this)]},void 0,!0,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\",lineNumber:1,columnNumber:1},this)}function vn(c={}){let{wrapper:r}=c.components||{};return r?(0,s.jsxDEV)(r,Object.assign({},c,{children:(0,s.jsxDEV)(Ce,c,void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\"},this)}),void 0,!1,{fileName:\"C:\\\\Users\\\\oisin\\\\Desktop\\\\blog\\\\personal_blog\\\\content\\\\_mdx_bundler_entry_point-1aeccd0b-579f-41ff-826b-3b980cd4d0bf.mdx\"},this):Ce(c)}var _n=vn;return pn(yn);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "in-need-of-assistants/index.mdx",
  "_raw": {
    "sourceFilePath": "in-need-of-assistants/index.mdx",
    "sourceFileName": "index.mdx",
    "sourceFileDir": "in-need-of-assistants",
    "contentType": "mdx",
    "flattenedPath": "in-need-of-assistants"
  },
  "type": "Blog",
  "url": "/blog/in-need-of-assistants",
  "readingTime": {
    "text": "6 min read",
    "minutes": 5.91,
    "time": 354600,
    "words": 1182
  },
  "toc": [
    {
      "level": "three",
      "text": "Developments & Changes",
      "slug": "developments--changes"
    },
    {
      "level": "three",
      "text": "GPTS",
      "slug": "gpts"
    },
    {
      "level": "three",
      "text": "Assistants API",
      "slug": "assistants-api"
    }
  ]
}